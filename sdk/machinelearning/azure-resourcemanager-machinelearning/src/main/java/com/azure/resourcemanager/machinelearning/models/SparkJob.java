// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.
// Code generated by Microsoft (R) AutoRest Code Generator.

package com.azure.resourcemanager.machinelearning.models;

import com.azure.core.annotation.Fluent;
import com.azure.core.util.logging.ClientLogger;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.annotation.JsonTypeInfo;
import com.fasterxml.jackson.annotation.JsonTypeName;
import java.util.List;
import java.util.Map;

/** Spark job definition. */
@JsonTypeInfo(use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = "jobType")
@JsonTypeName("Spark")
@Fluent
public final class SparkJob extends JobBaseProperties {
    /*
     * Archive files used in the job.
     */
    @JsonProperty(value = "archives")
    private List<String> archives;

    /*
     * Arguments for the job.
     */
    @JsonProperty(value = "args")
    private String args;

    /*
     * [Required] ARM resource ID of the code asset.
     */
    @JsonProperty(value = "codeId", required = true)
    private String codeId;

    /*
     * Spark configured properties.
     */
    @JsonProperty(value = "conf")
    @JsonInclude(value = JsonInclude.Include.NON_NULL, content = JsonInclude.Include.ALWAYS)
    private Map<String, String> conf;

    /*
     * [Required] The entry to execute on startup of the job.
     */
    @JsonProperty(value = "entry", required = true)
    private SparkJobEntry entry;

    /*
     * The ARM resource ID of the Environment specification for the job.
     */
    @JsonProperty(value = "environmentId")
    private String environmentId;

    /*
     * Environment variables included in the job.
     */
    @JsonProperty(value = "environmentVariables")
    @JsonInclude(value = JsonInclude.Include.NON_NULL, content = JsonInclude.Include.ALWAYS)
    private Map<String, String> environmentVariables;

    /*
     * Files used in the job.
     */
    @JsonProperty(value = "files")
    private List<String> files;

    /*
     * Mapping of input data bindings used in the job.
     */
    @JsonProperty(value = "inputs")
    @JsonInclude(value = JsonInclude.Include.NON_NULL, content = JsonInclude.Include.ALWAYS)
    private Map<String, JobInput> inputs;

    /*
     * Jar files used in the job.
     */
    @JsonProperty(value = "jars")
    private List<String> jars;

    /*
     * Mapping of output data bindings used in the job.
     */
    @JsonProperty(value = "outputs")
    @JsonInclude(value = JsonInclude.Include.NON_NULL, content = JsonInclude.Include.ALWAYS)
    private Map<String, JobOutput> outputs;

    /*
     * Python files used in the job.
     */
    @JsonProperty(value = "pyFiles")
    private List<String> pyFiles;

    /*
     * Queue settings for the job
     */
    @JsonProperty(value = "queueSettings")
    private QueueSettings queueSettings;

    /*
     * Compute Resource configuration for the job.
     */
    @JsonProperty(value = "resources")
    private SparkResourceConfiguration resources;

    /** Creates an instance of SparkJob class. */
    public SparkJob() {
    }

    /**
     * Get the archives property: Archive files used in the job.
     *
     * @return the archives value.
     */
    public List<String> archives() {
        return this.archives;
    }

    /**
     * Set the archives property: Archive files used in the job.
     *
     * @param archives the archives value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withArchives(List<String> archives) {
        this.archives = archives;
        return this;
    }

    /**
     * Get the args property: Arguments for the job.
     *
     * @return the args value.
     */
    public String args() {
        return this.args;
    }

    /**
     * Set the args property: Arguments for the job.
     *
     * @param args the args value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withArgs(String args) {
        this.args = args;
        return this;
    }

    /**
     * Get the codeId property: [Required] ARM resource ID of the code asset.
     *
     * @return the codeId value.
     */
    public String codeId() {
        return this.codeId;
    }

    /**
     * Set the codeId property: [Required] ARM resource ID of the code asset.
     *
     * @param codeId the codeId value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withCodeId(String codeId) {
        this.codeId = codeId;
        return this;
    }

    /**
     * Get the conf property: Spark configured properties.
     *
     * @return the conf value.
     */
    public Map<String, String> conf() {
        return this.conf;
    }

    /**
     * Set the conf property: Spark configured properties.
     *
     * @param conf the conf value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withConf(Map<String, String> conf) {
        this.conf = conf;
        return this;
    }

    /**
     * Get the entry property: [Required] The entry to execute on startup of the job.
     *
     * @return the entry value.
     */
    public SparkJobEntry entry() {
        return this.entry;
    }

    /**
     * Set the entry property: [Required] The entry to execute on startup of the job.
     *
     * @param entry the entry value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withEntry(SparkJobEntry entry) {
        this.entry = entry;
        return this;
    }

    /**
     * Get the environmentId property: The ARM resource ID of the Environment specification for the job.
     *
     * @return the environmentId value.
     */
    public String environmentId() {
        return this.environmentId;
    }

    /**
     * Set the environmentId property: The ARM resource ID of the Environment specification for the job.
     *
     * @param environmentId the environmentId value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withEnvironmentId(String environmentId) {
        this.environmentId = environmentId;
        return this;
    }

    /**
     * Get the environmentVariables property: Environment variables included in the job.
     *
     * @return the environmentVariables value.
     */
    public Map<String, String> environmentVariables() {
        return this.environmentVariables;
    }

    /**
     * Set the environmentVariables property: Environment variables included in the job.
     *
     * @param environmentVariables the environmentVariables value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withEnvironmentVariables(Map<String, String> environmentVariables) {
        this.environmentVariables = environmentVariables;
        return this;
    }

    /**
     * Get the files property: Files used in the job.
     *
     * @return the files value.
     */
    public List<String> files() {
        return this.files;
    }

    /**
     * Set the files property: Files used in the job.
     *
     * @param files the files value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withFiles(List<String> files) {
        this.files = files;
        return this;
    }

    /**
     * Get the inputs property: Mapping of input data bindings used in the job.
     *
     * @return the inputs value.
     */
    public Map<String, JobInput> inputs() {
        return this.inputs;
    }

    /**
     * Set the inputs property: Mapping of input data bindings used in the job.
     *
     * @param inputs the inputs value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withInputs(Map<String, JobInput> inputs) {
        this.inputs = inputs;
        return this;
    }

    /**
     * Get the jars property: Jar files used in the job.
     *
     * @return the jars value.
     */
    public List<String> jars() {
        return this.jars;
    }

    /**
     * Set the jars property: Jar files used in the job.
     *
     * @param jars the jars value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withJars(List<String> jars) {
        this.jars = jars;
        return this;
    }

    /**
     * Get the outputs property: Mapping of output data bindings used in the job.
     *
     * @return the outputs value.
     */
    public Map<String, JobOutput> outputs() {
        return this.outputs;
    }

    /**
     * Set the outputs property: Mapping of output data bindings used in the job.
     *
     * @param outputs the outputs value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withOutputs(Map<String, JobOutput> outputs) {
        this.outputs = outputs;
        return this;
    }

    /**
     * Get the pyFiles property: Python files used in the job.
     *
     * @return the pyFiles value.
     */
    public List<String> pyFiles() {
        return this.pyFiles;
    }

    /**
     * Set the pyFiles property: Python files used in the job.
     *
     * @param pyFiles the pyFiles value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withPyFiles(List<String> pyFiles) {
        this.pyFiles = pyFiles;
        return this;
    }

    /**
     * Get the queueSettings property: Queue settings for the job.
     *
     * @return the queueSettings value.
     */
    public QueueSettings queueSettings() {
        return this.queueSettings;
    }

    /**
     * Set the queueSettings property: Queue settings for the job.
     *
     * @param queueSettings the queueSettings value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withQueueSettings(QueueSettings queueSettings) {
        this.queueSettings = queueSettings;
        return this;
    }

    /**
     * Get the resources property: Compute Resource configuration for the job.
     *
     * @return the resources value.
     */
    public SparkResourceConfiguration resources() {
        return this.resources;
    }

    /**
     * Set the resources property: Compute Resource configuration for the job.
     *
     * @param resources the resources value to set.
     * @return the SparkJob object itself.
     */
    public SparkJob withResources(SparkResourceConfiguration resources) {
        this.resources = resources;
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withComponentId(String componentId) {
        super.withComponentId(componentId);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withComputeId(String computeId) {
        super.withComputeId(computeId);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withDisplayName(String displayName) {
        super.withDisplayName(displayName);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withExperimentName(String experimentName) {
        super.withExperimentName(experimentName);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withIdentity(IdentityConfiguration identity) {
        super.withIdentity(identity);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withIsArchived(Boolean isArchived) {
        super.withIsArchived(isArchived);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withNotificationSetting(NotificationSetting notificationSetting) {
        super.withNotificationSetting(notificationSetting);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withSecretsConfiguration(Map<String, SecretConfiguration> secretsConfiguration) {
        super.withSecretsConfiguration(secretsConfiguration);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withServices(Map<String, JobService> services) {
        super.withServices(services);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withDescription(String description) {
        super.withDescription(description);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withProperties(Map<String, String> properties) {
        super.withProperties(properties);
        return this;
    }

    /** {@inheritDoc} */
    @Override
    public SparkJob withTags(Map<String, String> tags) {
        super.withTags(tags);
        return this;
    }

    /**
     * Validates the instance.
     *
     * @throws IllegalArgumentException thrown if the instance is not valid.
     */
    @Override
    public void validate() {
        super.validate();
        if (codeId() == null) {
            throw LOGGER
                .logExceptionAsError(
                    new IllegalArgumentException("Missing required property codeId in model SparkJob"));
        }
        if (entry() == null) {
            throw LOGGER
                .logExceptionAsError(new IllegalArgumentException("Missing required property entry in model SparkJob"));
        } else {
            entry().validate();
        }
        if (inputs() != null) {
            inputs()
                .values()
                .forEach(
                    e -> {
                        if (e != null) {
                            e.validate();
                        }
                    });
        }
        if (outputs() != null) {
            outputs()
                .values()
                .forEach(
                    e -> {
                        if (e != null) {
                            e.validate();
                        }
                    });
        }
        if (queueSettings() != null) {
            queueSettings().validate();
        }
        if (resources() != null) {
            resources().validate();
        }
    }

    private static final ClientLogger LOGGER = new ClientLogger(SparkJob.class);
}
